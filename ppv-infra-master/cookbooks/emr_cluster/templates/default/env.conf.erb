spark {
  master = "yarn-client" 

  # Default # of CPUs for jobs to use for Spark standalone cluster
  job-number-cpus = 4

  jobserver {
    port = 8090
    jobdao = spark.jobserver.io.JobSqlDAO
    max-jobs-per-context = 40
    
    sqldao {
    }
  }

  # predefined Spark contexts
  contexts {
    test-context {
      num-cpu-cores = <%= @num_cpu_cores %> # shared tasks work best in parallel.
      memory-per-node = <%= @memory_per_node %> # trial-and-error discovered memory per node
      spark.executor.instances = <%= @executor_instances %> # 2 t2.medium instances with 2 cores each = 8 + 1 master
      spark.scheduler.mode = "FAIR"
      spark.task.maxFailures = 1   
    }
  }

  # universal context configuration.  These settings can be overridden, see README.md
  context-settings {
    spark.driver.port = 32456 # important

    # defines the place where your spark-assembly jar is located in your hdfs
    spark.yarn.jar = "hdfs://<%= @hdfs_name_node %>:8020/user/hadoop/spark_jars/spark-assembly-1.5.2-hadoop2.6.0-amzn-2.jar" # important

    num-cpu-cores = 2           # Number of cores to allocate.  Required.
    memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, #1G, etc.

    # uris of jars to be loaded into the classpath for this context. Uris is a string list, or a string separated by commas ','
    dependent-jar-uris = ["file://<%= @install_dir %>/sqljdbc41.jar"]
    
    # If you wish to pass any settings directly to the sparkConf as-is, add them here in passthrough,
    # such as hadoop connection settings that don't use the "spark." prefix
    passthrough {
      configPath="/opt/periscope/application.conf"
    }

  }

  # This needs to match SPARK_HOME for cluster SparkContexts to be created successfully
  home = <%= @spark_home %>
}